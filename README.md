# Decoder only transformer

This project is based off of Nano-GPT from Andrej Karpathy, and the GPT2, GPT3, and Attention Is All You Need papers. It's a decoder only transformer that is trained off of a single text file (Shakespear in this example). GPU is highly recommended for training.

*This project was made for educational purposes*
