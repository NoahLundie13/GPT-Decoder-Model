# Decoder only transformer

This project is based off of Nano-GPT from Andrej Karpathy, and the GPT2, GPT3, and Attention Is All You Need papers. It's a decoder only transformer that is trained off of a single text file (Shakespear in this example). GPU is highly recommended for training.

~10M parameters

I also experimented with Claude 3.7 to create a visualization software for the model and it's awesome! It's all in a standalone html file and you can host it locally to check it out.

![Visualizer Image](/visualizer.png?raw=true "Visualizer")


*This project was made for educational purposes*
